The file URL is given below:
https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt

The server access log file contains these fields.

a. timestamp - TIMESTAMP
b. latitude - float
c. longitude - float
d. visitorid - char(37)
e. accessed_from_mobile - boolean
f. browser_code - int

Tasks
Add tasks in the DAG file to download the file, read the file, and extract the fields timestamp and visitorid from the web-server-access-log.txt.

Capitalize the visitorid for all the records and store it in a local variable.

Load the data into a new file capitalized.txt.

Create the imports block.

Create the DAG Arguments block. You can use the default settings.

Create the DAG definition block. The DAG should run daily.

Create the tasks extract, transform, and load to call the Python script.

Create the task pipeline block.

Submit the DAG.

Verify if the DAG is submitted.